# API

sparta.api.host = 0.0.0.0
sparta.api.port = 9090
sparta.api.certificate-file = "/home/user/certifications/stratio.jks"
sparta.api.certificate-password = "stratio"


# ZOOKEEPER

sparta.zookeeper.connectionString = "localhost:2181"
sparta.zookeeper.connectionTimeout = 15000
sparta.zookeeper.sessionTimeout = 60000
sparta.zookeeper.retryAttempts = 5
sparta.zookeeper.retryInterval = 10000


# OAUTH2

oauth2.enable = "false"
# oauth2.cookieName="user"
# oauth2.url.authorize = "https://server.domain:9005/cas/oauth2.0/authorize"
# oauth2.url.accessToken = "https://server.domain:9005/cas/oauth2.0/accessToken"
# oauth2.url.profile = "https://server.domain:9005/cas/oauth2.0/profile"
# oauth2.url.logout = "https://server.domain:9005/cas/logout"
# oauth2.url.callBack = "http://callback.domain:9090/login"
# oauth2.url.onLoginGoTo = "/"
# oauth2.client.id = "userid"
# oauth2.client.secret = "usersecret"


# SPRAY

spray.can.server.ssl-encryption = "off"


# AKKA

akka.log-dead-letters = off
akka.loggers = ["akka.event.slf4j.Slf4jLogger"]
akka.logger-startup-timeout = 30s


# CONFIG

# The execution modes in Sparta are: local, mesos or marathon
sparta.config.executionMode = local

# Driver jar served by Sparta in this location
sparta.config.driverPackageLocation = "/opt/sds/sparta/driver/"

# Plugin jar are saved in this dir
sparta.config.pluginPackageLocation = "/opt/sds/sparta/plugins/"

# The jar driver location in Sparta: hdfs, http or local
sparta.config.driverURI = "http://0.0.0.0:9090/driver/sparta-driver.jar"

# The rememberPartitioner for RDD generated in Stateful operations in Spark, like UpdateStateByKey
sparta.config.rememberPartitioner = true

# Maximun time when await to policy status change when run polices in cluster mode
sparta.config.awaitPolicyChangeStatus = 180s

# Example value in local mode for debugging
sparta.config.checkpointPath = "/tmp/sparta/checkpoint"

# Auto delete checkpoint when run policies in cluster mode is necessary HDFS configuration
sparta.config.autoDeleteCheckpoint = true

# Add time to checkpoint path when run policies in cluster mode is necessary HDFS configuration
sparta.config.addTimeToCheckpointPath = false

# FRONTEND
sparta.config.frontend.timeout = 5000

####################################
#                                  #
#      Security configuration      #
#                                  #
####################################

# Sparta security mode implementation based on the GoSec plugin
sparta.security.manager.enabled = false


# HDFS

# The hadoop user name could be configured by two ways:
# 1. Using the enviroment variable HADOOP_USER_NAME
# 2. Using the variable hadoopUserName from properties file
# sparta.hdfs.hadoopUserName = root

# If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
# connect to HDFS cluster in order to upload jars to HDFS (plugins and driver), but the Spark executors and the
# Spark driver need this environment variable defined. In producction environments is recomended use
# HADOOP_CONF_DIR because use HA in Hadoop Namenodes, and omit "hdfsMaster and hdfsPort property"
# sparta.hdfs.hdfsMaster = hadoopNameNodeAddress
# sparta.hdfs.hdfsPort = 9000

# Configuration to connect to HDFS Kerberized

# The principal name could be configured by three ways:
# 1. Using the enviroment variable SPARTA_PRINCIPAL_NAME
# 2. Using the variables "principalNamePrefix" and "principalNameSuffix" and the enviroment variable HOSTNAME
# The principal name used to connect to HDFS securized have the order 1, 2
# sparta.hdfs.principalName = ""


# The keytab path could be configured by two ways:
# 1. Using the enviroment variable SPARTA_KEYTAB_PATH
# 2. Using the variable keytabPath from properties file

# sparta.hdfs.keytabPath = ""
# sparta.hdfs.reloadKeyTab = false
# sparta.hdfs.reloadKeyTabTime = 23h



### SPARK MESOS deployments ###

# If the environment variable SPARK_HOME is defined, the spark submit use the variable
# sparta.mesos.sparkHome = ""

# Application Name, is possible to assign with "name" and the property spark.app.name.
# spark.app.name = SPARTA

# Coarse mode is recommended in Streaming Applications over Mesos Clusters. If is assinged false is used the
# Spark fine grained mode
sparta.mesos.spark.mesos.coarse = true

# Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
# variable spark.submit.deployMode
sparta.mesos.deployMode = cluster

# In Cluster mode is the SparkMesosDispatcher URI(master:7077), in Client mode the Mesos Master URI(master:5050)
# sparta.mesos.master = "mesos://mesosURI"

# Url to kill submissions in Spark with Spark Dispatcher API
# sparta.mesos.killUrl = "http://mesosDispatcherURL/v1/submissions/kill"

# Principal name used when run Sparta over Mesos securized
# sparta.mesos.spark.mesos.principal=dcs1

# Secret password used when run Sparta over Mesos securized
# sparta.mesos.spark.mesos.secret=dcs_1_password

# Role assigned to spark jobs when run Sparta over Mesos securized
# sparta.mesos.spark.mesos.role=sparta

# Comma-separated list of local jars to include on the driver and executor classpaths
# sparta.mesos.jars = ""

# Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
# the local maven repo, then maven central and any additional remote repositories given by --repositories. The
# format for the coordinates should be groupId:artifactId:version
# sparta.mesos.packages = "com:stratio:package"

# Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
# to avoid dependency conflicts
# sparta.mesos.exclude-packages = "com:stratio:package"

# Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
# sparta.mesos.repositories = "hdfs://repo"

# User to impersonate when submitting the application. This argument does not work with --principal / --keytab
# sparta.mesos.proxy-user = username

# Extra Java options to pass to the driver
# sparta.mesos.driver-java-options = ""

# Extra library path entries to pass to the driver
# sparta.mesos.driver-library-path = ""

# Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
# the classpath
# sparta.mesos.driver-class-path = ""

# Memory by executor
sparta.mesos.spark.executor.memory = 1G

# Total cores by executors, is possible to assign the varibale spark.cores.max.
# If you donâ€™t set spark.cores.max, the Spark application will reserve all resources offered to it by Mesos
sparta.mesos.totalExecutorCores = 2

# IMPORTANT in SPARK 2.x!! Number of executors in Mesos: spark.cores.max/spark.executor.cores
# Cores by executor only in SPARK 2.x
sparta.mesos.spark.executor.cores = 1

# Cores assigned to Spark Driver. Cluster mode only
sparta.mesos.spark.driver.cores = 1

# Memory assigned to Spark Driver.
sparta.mesos.spark.driver.memory = 1G

# Extra amount of cpus to request per task
# sparta.mesos.spark.mesos.extra.cores = 0

# Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
# sparta.mesos.supervise = false

# Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
# takes processing time slightly more than batch interval.
# By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
# and till its not finished,other jobs will be queued up even if the resources are available and idle.
# sparta.mesos.spark.streaming.concurrentJobs = 1

# Stop gracefully the streaming contexts when shutdown the spark applications
# spark.streaming.stopGracefullyOnShutdown = true

# Turn this down to prevent long blocking at shutdown
# sparta.mesos.spark.streaming.gracefulStopTimeout = 100000

# Use other serializer than default Java Serializer
# sparta.mesos.spark.serializer = org.apache.spark.serializer.KryoSerializer

# Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
# sparta.mesos.propertiesFile = ""

# Run over Mesos Clusters with Docker conatiners
# sparta.mesos.spark.mesos.executor.docker.image="stratio/mesos-spark-2.1-scala-2.11-executor-dcos"

# Force pull image for each execution
# sparta.mesos.spark.mesos.executor.docker.forcePullImage=true

# Mount volumes from host machine in the executor docker container
# sparta.mesos.spark.mesos.executor.docker.volumes="/opt/mesosphere/packages/:/opt/mesosphere/packages/:ro,/opt/mesosphere/lib/:/opt/mesosphere/lib/:ro"

# Send environment variables to executor docker containers only in Spark slaves
# sparta.mesos.spark.executorEnv.MESOS_NATIVE_JAVA_LIBRARY="/opt/mesosphere/lib/libmesos.so"

# If Spark in Mesos Slaves are installed in a different path than spark submit installation
# sparta.mesos.spark.mesos.executor.home=/opt/spark/dist

# If you want to use one Spark compilation
# sparta.mesos.spark.executor.uri=" http://tools.stratio.com/spark/spark-mesosphere-scala211-1.6.2-bin-hadoop2.6.0.tgz"

# A comma-separated list of URIs to be downloaded to the sandbox when driver or executor is launched by Mesos
# sparta.mesos.spark.mesos.uris="http://hadoopNamenode:8020/conf/core-site.xml"

# Download HDFS conf in stratio spark fork
# sparta.mesos.spark.mesos.driverEnv.HDFS_CONF_URI=/opt/sds/hadoop/conf

# Increase job parallelity by reducing Spark Delay Scheduling (potentially big performance impact (!)) (Default: 3s)
# sparta.mesos.spark.locality.wait=10

# Increase max task failures before failing job (Default: 4)
# sparta.mesos.spark.task.maxFailures=8

# Tweak to balance data processing parallelism vs. task scheduling overhead (Default: 200ms)
# sparta.mesos.spark.streaming.blockInterval=200

# Available in all execution modes
# sparta.mesos.spark.driver.extraClassPath = ""
# sparta.mesos.spark.driver.extraJavaOptions = ""
# sparta.mesos.spark.driver.extraLibraryPath = ""
# sparta.mesos.spark.jars = ""
# sparta.mesos.spark.files = ""

# Available in client mode
# sparta.mesos.spark.jars.ivy = ""

# Parquet prevention errors
# sparta.mesos.spark.sql.parquet.binaryAsString = true

# How long to wait to launch a data-local task before giving up and launching it on a less-local node.
#sparta.mesos.spark.locality.wait = 10s

# Number of failures of any particular task before giving up on the job.
#sparta.mesos.spark.task.maxFailures = 8

# Interval at which data received by Spark Streaming receivers is chunked into blocks of data before storing them in Spark
#sparta.mesos.spark.streaming.blockInterval = 200ms

# Spark User in dispatcher deployments
# sparta.mesos.spark.mesos.driverEnv.SPARK_USER="sparta"


# MARATHON deployments

sparta.marathon.docker.image = "qa.stratio.com/stratio/sparta:1.4.0-SNAPSHOT"
sparta.marathon.docker.forcePullImage = false
sparta.marathon.docker.privileged = false
sparta.marathon.jar = "/opt/sds/sparta/driver/sparta-driver.jar"
sparta.marathon.template.file = "/etc/sds/sparta/marathon-app-template.json"
sparta.marathon.mesosphere.lib = "/opt/mesosphere/lib"
sparta.marathon.mesosphere.packages = "/opt/mesosphere/packages"
sparta.marathon.gracePeriodSeconds = 180
sparta.marathon.intervalSeconds = 60
sparta.marathon.timeoutSeconds = 20
sparta.marathon.maxConsecutiveFailures = 3
#sparta.marathon.sso.uri = "https://example.com"
#sparta.marathon.sso.username = "username"
#sparta.marathon.sso.password = "*****"
#sparta.marathon.sso.clientId = "clientId"
#sparta.marathon.sso.redirectUri = "https://app.com/acs/api/v1/auth/login&firstUser=false"
#sparta.marathon.tikitakka.marathon.uri = "http://localhost:8080"
sparta.marathon.tikitakka.marathon.api.version = "v2"



### SPARK LOCAL deployments ###

sparta.local.spark.app.name = SPARTA
sparta.local.spark.master = "local[*]"
sparta.local.spark.driver.memory = 1G
sparta.local.spark.executor.memory = 1G
# sparta.local.spark.metrics.conf = /opt/sds/sparta/benchmark/src/main/resources/metrics.properties
