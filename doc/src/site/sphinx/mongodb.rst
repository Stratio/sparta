MongoDB Specifications
******************

- :ref:`introduction-label`

- :ref:`driver-label`

- :ref:`worker-label`


.. _introduction-label:

Introduction
============

This output uses the native driver for MongoDB in Scala language. **Casbah Driver** offers all the functionality that
Sparkta need to insert,update and add and configure the database for proper system operation.

In the implementation of a Output on the Sparkta SDK there are two possibilities, one would be allowing the system
to transform our data even DataFrame type Spark or inserted directly a **UpdateMetricOperation**. This output does not
use any Spark plugin to insert DataFrames.

This plugin create one client connexion per Worker in a Spark Cluster.

Is necessary need override two functions of Output SDK:
::
  override def doPersist(stream: DStream[UpdateMetricOperation]): Unit
  override def upsert(metricOperations: Iterator[UpdateMetricOperation]): Unit


.. _driver-label:

Driver Operations
============

When the driver starts the Output, several processes for creating indexes needed to run the various collections.
There are two types of indexes:

  * Unique index:

    Create primary key that contains a field called "id" values separated by "_" and the field timeBucket buckets.
    This index is optional, because if you do not specify timeBucket not created and the primary key of the
    collection is "_id" field

  * Text index:

    If we do a text index on any bucket or some aggregate field, the system will create it for us.

      - Example:
      ::

        [
          {
            "v" : 1,
            "key" : {
              "_id" : 1
            },
            "name" : "_id_",
            "ns" : "sparkta.precision3_hastags_retweets_minute"
          },
          {
            "v" : 1,
            "key" : {
              "_fts" : "text",
              "_ftsx" : 1
            },
            "name" : "userLocation",
            "ns" : "sparkta.precision3_hastags_retweets_minute",
            "background" : true,
            "default_language" : "english",
            "weights" : {
              "userLocation" : 1
            },
            "language_override" : "language",
            "textIndexVersion" : 2
          },
          {
            "v" : 1,
            "unique" : true,
            "key" : {
              "id" : 1,
              "minute" : 1
            },
            "name" : "id_minute",
            "ns" : "sparkta.precision3_hastags_retweets_minute",
            "background" : true
          }
        ]


.. _worker-label:

Worker Operations
============

As this Output does not use functionality of DataFrames, override the method Upsert, that save all values
of a **UpdateMetricOperation**.
Below you can see each of the features implemented:

  * Each Worker save in one BulkOperation for each data partition of a RDD.

  * The output create one collection for each rollup. With the name "bucket1_bucket2..." + Optional(timeBucket if is
    specified in properties)

      - Example:
      ::

          hastags_minute
          hastags_retweets_minute
          hastags_retweets_urls_minute
          hastags_urls_minute
          precision3_hastags_minute
          precision3_hastags_retweets_minute
          precision3_hastags_retweets_urls_minute
          precision3_hastags_urls_minute
          precision3_minute
          precision3_retweets_minute
          precision3_retweets_urls_minute
          precision3_urls_minute
          retweets_minute
          retweets_urls_minute
          system.indexes
          urls_minute


  * The output upsert documents with the _id field "bucket1_bucket2...". If timeBucket
    is specified in properties the system save the data in two fields "id" with the buckets values and timeBucket
    field with the dateTime of the document. With the second the _id is autogenerated.

      - Example:
      ::

          "_id" : ObjectId("554891b3da00bdd0c284a573"),
          "id" : "List(0.703125, 0.703125)_1_0",
          "minute" : ISODate("2015-05-05T09:47:00Z"),
          "min_wordsN" : 1,
          "stddev_wordsN" : 2.8284271247461903,
          "avg_wordsN" : 6,
          "language" : "english",
          "variance_wordsN" : 8,
          "last_retweets" : NumberLong(0),
          "median_wordsN" : 6,
          "count" : NumberLong(750),
          "sum_wordsN" : NumberLong(7669),
          "max_wordsN" : 29,


  * MongoDB have several **Update Aggregation Commands** that are used by Sparkta for insert the aggregate fields. As
   can be Sum, Count, Avg, Max, Min.
