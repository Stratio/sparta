[
  {
    "name": "Cassandra",
    "modelType": "Cassandra",
    "description": {
      "short": "The Cassandra output uses the generic implementation with DataFrames.",
      "long": "The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Cassandra"
    },
    "properties": [
      {
        "propertyId": "connectionHost",
        "propertyName": "_CONTACT_POINT_",
        "propertyType": "text",
        "regexp": "((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(((?![0-9]+$)(?!.*-$)(?!-)[a-zA-Z0-9-]{2,63}))",
        "default": "localhost",
        "required": true,
        "tooltip": "Host",
        "qa": "fragment-details-cassandra-connectionHost"
      },
      {
        "propertyId": "connectionPort",
        "propertyName": "_PORT_",
        "propertyType": "text",
        "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
        "default": "9042",
        "required": true,
        "tooltip": "Cassandra port",
        "hidden": false,
        "qa": "fragment-details-cassandra-connectionPort"
      },
      {
        "propertyId": "keyspace",
        "propertyName": "_KEYSPACE_",
        "propertyType": "text",
        "regexp": "",
        "default": "sparta",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-cassandra-keyspace"
      },
      {
        "propertyId": "cluster",
        "propertyName": "_CLUSTER_",
        "propertyType": "text",
        "regexp": "",
        "default": "sparta",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-cassandra-cluster"
      },
      {
        "propertyId": "keyspaceClass",
        "propertyName": "_KEYSPACE_CLASS_",
        "propertyType": "select",
        "regexp": "simpleStrategy|networkTopologyStrategy",
        "values": [
          {
            "label": "SimpleStrategy",
            "value": "simpleStrategy"
          },
          {
            "label": "NetworkTopologyStrategy",
            "value": "networkTopologyStrategy"
          }
        ],
        "default": "simpleStrategy",
        "required": true,
        "tooltip": "SimpleStrategy recommended for a single data center.",
        "qa": "fragment-details-cassandra-keyspaceClass"
      },
      {
        "propertyId": "replication_factor",
        "propertyName": "_REPLICATION_FACTOR_",
        "propertyType": "text",
        "regexp": "\\d*",
        "default": "1",
        "required": true,
        "tooltip": "Specifies the number of replicas of data on multiple nodes. Only valid if SimpleStrategy is chosen.",
        "qa": "fragment-details-cassandra-replication-factor"
      },
      {
        "propertyId": "compactStorage",
        "propertyName": "_COMPACT_STORAGE_",
        "propertyType": "boolean",
        "regexp": "true|false",
        "default": false,
        "required": false,
        "tooltip": "The compact storage directive is used for backward compatibility of CQL 2 applications and data in the legacy (Thrift) storage engine format. To take advantage of CQL 3 capabilities, do not use this directive in new applications. When you create a table using compound primary keys, for every piece of data stored, the column name needs to be stored along with it. Instead of each non-primary key column being stored such that each column corresponds to one column on disk, an entire row is stored in a single column on disk, hence the name compact storage.",
        "qa": "fragment-details-cassandra-compactStorage"
      },
      {
        "propertyId": "analyzer",
        "propertyName": "_LUCENE_ANALYZER_",
        "propertyType": "text",
        "regexp": "",
        "default": "english",
        "required": false,
        "tooltip": "The analyzer for text index fields. This feature is for the Stratio’s Cassandra Lucene Index",
        "qa": "fragment-details-cassandra-analyzer"
      },
      {
        "propertyId": "refreshSeconds",
        "propertyName": "_REFRESH_SECONDS_",
        "propertyType": "text",
        "regexp": "\\d*",
        "default": "1",
        "required": false,
        "tooltip": "The number of seconds between every refresh for index operations. This feature is for the Stratio’s Cassandra Lucene Index",
        "qa": "fragment-details-cassandra-refreshSeconds"
      },
      {
        "propertyId": "dateFormat",
        "propertyName": "_DATE_FORMAT_",
        "propertyType": "select",
        "regexp": "",
        "values": [
          {
            "label": "yyyy-mm-dd HH:mm",
            "value": "yyyy-mm-dd HH:mm"
          },
          {
            "label": "yyyy-mm-dd HH:mm:ss",
            "value": "yyyy-mm-dd HH:mm:ss"
          },
          {
            "label": "yyyy-mm-dd HH:mmZ",
            "value": "yyyy-mm-dd HH:mmZ"
          },
          {
            "label": "yyyy-mm-dd HH:mm:ssZ",
            "value": "yyyy-mm-dd HH:mm:ssZ"
          },
          {
            "label": "yyyy-mm-dd'T'HH:mm",
            "value": "yyyy-mm-dd'T'HH:mm"
          },
          {
            "label": "yyyy-mm-dd'T'HH:mmZ",
            "value": "yyyy-mm-dd'T'HH:mmZ"
          },
          {
            "label": "yyyy-mm-dd'T'HH:mm:ss",
            "value": "yyyy-mm-dd'T'HH:mm:ss"
          },
          {
            "label": "yyyy-mm-dd'T'HH:mm:ssZ",
            "value": "yyyy-mm-dd'T'HH:mm:ssZ"
          },
          {
            "label": "yyyy-mm-dd",
            "value": "yyyy-mm-dd"
          },
          {
            "label": "yyyy-mm-ddZ",
            "value": "yyyy-mm-ddZ"
          }
        ],
        "default": "yyyy-mm-dd HH:mm",
        "required": false,
        "tooltip": "The date format for the indexed date fields. This feature is for the Stratio’s Cassandra Lucene Index",
        "qa": "fragment-details-cassandra-dateFormat"
      },
      {
        "propertyId": "sparkProperties",
        "propertyName": "_SPARK_PROPERTIES_",
        "propertyType": "list",
        "default": "",
        "required": false,
        "tooltip": "",
        "qa": "fragment-details-cassandra-spark",
        "fields": [
          {
            "propertyId": "sparkPropertyKey",
            "propertyName": "_SPARK_PROPERTY_KEY_",
            "propertyType": "text",
            "regexp": "",
            "default": "spark.cassandra.connection.keep_alive_ms",
            "hidden": false,
            "required": false,
            "qa": "fragment-details-cassandra-sparkPropertyKey"
          },
          {
            "propertyId": "sparkPropertyValue",
            "propertyName": "_SPARK_PROPERTY_VALUE_",
            "propertyType": "text",
            "regexp": "",
            "default": "180000",
            "hidden": false,
            "required": true,
            "qa": "fragment-details-cassandra-sparkPropertyValue"
          }
        ]
      }
    ]
  },
  {
    "name": "CSV",
    "modelType": "Csv",
    "description": {
      "short": "Persist your data in HDFS with CSV format.",
      "long": "Persist your data in HDFS with CSV format.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-CSV"
    },
    "properties": [
      {
        "propertyId": "path",
        "propertyName": "_PATH_",
        "propertyType": "text",
        "regexp": "^[a-zA-Z0-9:_\\.\\-\\+/\\\\]{2,}\\.csv$",
        "default": "path_to_csv.csv",
        "required": true,
        "qa": "fragment-details-csv-path"
      },
      {
        "propertyId": "header",
        "propertyName": "_HEADER_",
        "propertyType": "boolean",
        "regexp": "true|false",
        "default": false,
        "required": false,
        "tooltip": "Sets if a header is attached to the output file.",
        "qa": "fragment-details-csv-header"
      },
      {
        "propertyId": "inferSchema",
        "propertyName": "_INFER_SCHEMA_",
        "propertyType": "boolean",
        "regexp": "true|false",
        "default": false,
        "required": false,
        "tooltip": "If checked, the data schema will be attached to the output file.",
        "qa": "fragment-details-csv-inferSchema"
      },
      {
        "propertyId": "delimiter",
        "propertyName": "_DELIMITER_",
        "propertyType": "text",
        "tooltip": "Any character is accepted except: \\ \" #",
        "regexp": "[^\"#\\\\]",
        "maxlength": "1",
        "default": ",",
        "trim": false,
        "required": false,
        "qa": "fragment-details-csv-delimiter"
      }
    ]
  },
  {
    "name": "Elasticsearch",
    "modelType": "ElasticSearch",
    "description": {
      "short": "The Elasticsearch output uses the generic implementation with DataFrames.",
      "long": "Elasticsearch is a search server based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Elasticsearch"
    },
    "properties": [
      {
        "propertyId": "nodes",
        "propertyName": "_NODES_",
        "propertyType": "list",
        "default": "",
        "required": true,
        "hidden": false,
        "limit": 0,
        "tooltip": "",
        "qa": "fragment-details-elasticSearch-nodes",
        "fields": [
          {
            "propertyId": "node",
            "propertyName": "_HOST_",
            "propertyType": "text",
            "regexp": "((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(((?![0-9]+$)(?!.*-$)(?!-)[a-zA-Z0-9-]{2,63}))",
            "default": "localhost",
            "required": true,
            "qa": "fragment-details-elasticSearch-node"
          },
          {
            "propertyId": "tcpPort",
            "propertyName": "_TCP_PORT_",
            "propertyType": "text",
            "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
            "default": "9300",
            "tooltip": "Port to bind for communication between nodes.",
            "required": true,
            "qa": "fragment-details-elasticSearch-tcpPort"
          },
          {
            "propertyId": "httpPort",
            "propertyName": "_HTTP_PORT_",
            "propertyType": "text",
            "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
            "default": "9200",
            "required": true,
            "qa": "fragment-details-elasticSearch-httpPort"
          }
        ]
      },
      {
        "propertyId": "clusterName",
        "propertyName": "_CLUSTERNAME_",
        "propertyType": "text",
        "regexp": "",
        "default": "elasticsearch",
        "required": true,
        "qa": "fragment-details-elasticSearch-clusterName"
      },
      {
        "propertyId": "idField",
        "propertyName": "_ID_FIELD_",
        "propertyType": "text",
        "regexp": "",
        "default": "",
        "tooltip": "Sets the key used to look for more coincidences and save by upserting.",
        "required": false,
        "qa": "fragment-details-elasticSearch-idField"
      },
      {
        "propertyId": "indexMapping",
        "propertyName": "_INDEX_MAPPING_",
        "propertyType": "text",
        "regexp": "",
        "default": "sparta",
        "tooltip": "Value will be used as mapping name for indexing.",
        "required": false,
        "qa": "fragment-details-elasticSearch-indexMapping"
      }
    ]
  },
  {
    "name": "MongoDb",
    "modelType": "MongoDb",
    "description": {
      "short": "MongoDB is an open-source document database, and the leading NoSQL database.",
      "long": "MongoDB is an open-source document database that provides high performance, high availability, and automatic scaling.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-MongoDB"
    },
    "properties": [
      {
        "propertyId": "hosts",
        "propertyName": "_HOSTS_",
        "propertyType": "list",
        "default": "",
        "required": true,
        "tooltip": "This section let us specify the parameters needed to establish a MongoDB connection, with different replica set or with sharding.",
        "qa": "fragment-details-mongoDb-hosts",
        "fields": [
          {
            "propertyId": "host",
            "propertyName": "_HOST_",
            "propertyType": "text",
            "regexp": "((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(((?![0-9]+$)(?!.*-$)(?!-)[a-zA-Z0-9-]{2,63}))",
            "default": "localhost",
            "hidden": false,
            "required": true,
            "qa": "fragment-details-mongoDb-hostName"
          },
          {
            "propertyId": "port",
            "propertyName": "_PORT_",
            "propertyType": "text",
            "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
            "default": "27017",
            "hidden": false,
            "required": true,
            "qa": "fragment-details-mongoDb-port"
          }
        ]
      },
      {
        "propertyId": "dbName",
        "propertyName": "_DATEBASE_NAME_",
        "propertyType": "text",
        "regexp": "",
        "default": "sparta",
        "required": true,
        "tooltip": "Default value set to sparta.",
        "qa": "fragment-details-mongoDb-dbName"
      },
      {
        "propertyId": "connectionsPerHost",
        "propertyName": "_CONNECTIONS_PER_HOST_",
        "propertyType": "text",
        "regexp": "\\d*",
        "default": "5",
        "required": false,
        "tooltip": "Default value set to 5.",
        "qa": "fragment-details-mongoDb-connectionsPerHost"
      },
      {
        "propertyId": "threadsAllowedToBlock",
        "propertyName": "_THREADS_ALLOWED_TO_BLOCK_",
        "propertyType": "text",
        "regexp": "\\d*",
        "default": "10",
        "required": false,
        "tooltip": "This value multiplied by the connectionsPerHost field specifies the maximum number of threads that may be waiting for a connection to become available from the pool.",
        "qa": "fragment-details-mongoDb-threadsAllowedToBlock"
      },
      {
        "propertyId": "language",
        "propertyName": "_LANGUAGE_",
        "propertyType": "select",
        "regexp": "",
        "values": [
          {
            "label": "danish",
            "value": "danish"
          },
          {
            "label": "dutch",
            "value": "dutch"
          },
          {
            "label": "english",
            "value": "english"
          },
          {
            "label": "finnish",
            "value": "finnish"
          },
          {
            "label": "french",
            "value": "french"
          },
          {
            "label": "german",
            "value": "german"
          },
          {
            "label": "hungarian",
            "value": "hungarian"
          },
          {
            "label": "italian",
            "value": "italian"
          },
          {
            "label": "norwegian",
            "value": "norwegian"
          },
          {
            "label": "portuguese",
            "value": "portuguese"
          },
          {
            "label": "romanian",
            "value": "romanian"
          },
          {
            "label": "russian",
            "value": "russian"
          },
          {
            "label": "spanish",
            "value": "spanish"
          },
          {
            "label": "swedish",
            "value": "swedish"
          },
          {
            "label": "turkish",
            "value": "turkish"
          }
        ],
        "default": "",
        "required": false,
        "tooltip": "Specifies the tokenizer language in the full-text index in MongoDB, each document inserted must have this key-value.",
        "qa": "fragment-details-mongoDb-language"
      },
      {
        "propertyId": "retrySleep",
        "propertyName": "_RETRY_SLEEP_",
        "propertyType": "text",
        "regexp": "\\d*",
        "default": "1000",
        "required": false,
        "tooltip": "The number of milliseconds to wait before reconnecting with the MongoDB nodes when the last client fails.",
        "qa": "fragment-details-mongoDb-retrySleep"
      }
    ]
  },
  {
    "name": "Parquet",
    "modelType": "Parquet",
    "description": {
      "short": "Parquet output uses the generic implementation with DataFrames.",
      "long": "Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/latSPARTA0x11est/Outputs#Outputs-Parquet"
    },
    "properties": [
      {
        "propertyId": "path",
        "propertyName": "_PATH_",
        "propertyType": "text",
        "regexp": "",
        "default": "",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-parquet-path"
      },
      {
        "propertyId": "partitionBy",
        "propertyName": "_PARTITION_BY_",
        "propertyType": "text",
        "regexp": "",
        "required": false,
        "tooltip": "Partitions the output by the specified column on the file system",
        "qa": "fragment-details-parquet-partitionBy"
      }
    ]
  },
  {
    "name": "Print",
    "modelType": "Print",
    "description": {
      "short": "Print output uses the generic implementation with DataFrames, this implementation print each dataframe with his schema.",
      "long": "Print output uses the generic implementation with DataFrames, this implementation print each dataframe with his schema.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Print"
    },
    "properties": [
    ]
  },
  {
    "name": "Redis",
    "modelType": "Redis",
    "description": {
      "short": "The output of Redis use the generic implementation with DataFrames.",
      "long": "The output of Redis use the generic implementation with DataFrames. Redis is an open source (BSD licensed), in-memory data structure store, used as database, cache and message broker.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Redis"
    },
    "properties": [
      {
        "propertyId": "hostname",
        "propertyName": "_HOST_",
        "propertyType": "text",
        "regexp": "((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(((?![0-9]+$)(?!.*-$)(?!-)[a-zA-Z0-9-]{2,63}))",
        "default": "localhost",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-redis-hostname"
      },
      {
        "propertyId": "port",
        "propertyName": "_PORT_",
        "propertyType": "text",
        "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
        "default": "6379",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-redis-port"
      }
    ]
  },
  {
  "name": "Kafka",
  "modelType": "Kafka",
  "description": {
    "short": "Apache Kafka is publish-subscribe messaging rethought as a distributed commit log.",
    "long": "Apache Kafka is publish-subscribe messaging rethought as a distributed commit log. Based on the received-based approach (https://spark.apache.org/docs/latest/streaming-kafka-integration.html)",
    "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Kafka"
  },
  "properties": [
    {
      "propertyId": "metadata.broker.list",
      "propertyName": "_METADATA_BROKER_LIST_",
      "propertyType": "list",
      "regexp": "",
      "default": "",
      "required": true,
      "tooltip": "Kafka host/port to connect",
      "qa": "fragment-details-kafka-metadata-broker-list",
      "fields": [
        {
          "propertyId": "host",
          "propertyName": "_HOST_",
          "propertyType": "text",
          "regexp": "((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(((?![0-9]+$)(?!.*-$)(?!-)[a-zA-Z0-9-]{2,63}))",
          "default": "localhost",
          "required": true,
          "tooltip": "Kafka's address.",
          "hidden": false,
          "qa": "fragment-details-kafka-host"
        },
        {
          "propertyId": "port",
          "propertyName": "_PORT_",
          "propertyType": "text",
          "regexp": "(0|([1-9]\\d{0,3}|[1-5]\\d{4}|[6][0-5][0-5]([0-2]\\d|[3][0-5])))",
          "default": "9092",
          "required": true,
          "tooltip": "Kafka's port.",
          "hidden": false,
          "qa": "fragment-details-kafka-port"
        }
      ]
    },
    {
      "propertyId": "serializer.class",
      "propertyName": "_SERIALIZER_",
      "propertyType": "text",
      "regexp": "",
      "default": "kafka.serializer.StringEncoder",
      "required": true,
      "tooltip": "",
      "qa": "fragment-details-kafka-serializer-class"
    },
    {
      "propertyId": "request.required.acks",
      "propertyName": "_REQUIRED_ACKS_",
      "propertyType": "boolean",
      "regexp": "true|false",
      "default": false,
      "required": false,
      "tooltip": "Specifies whether the producer waits for an acknowledgment from the broker.",
      "qa": "fragment-details-kafka-request-required-acks"
    },
    {
      "propertyId": "producer.type",
      "propertyName": "_PRODUCER_TYPE_",
      "propertyType": "text",
      "regexp": "async|sync",
      "default": "async",
      "required": true,
      "tooltip": "Specifies whether the messages are sent asynchronously in a background thread. Set to sync by default",
      "qa": "fragment-details-kafka-producer-type"
    },
    {
      "propertyId": "batch.num.messages",
      "propertyName": "_BATCH_NUM_MESSAGES_",
      "propertyType": "text",
      "regexp": "[0-9]+",
      "default": "200",
      "required": true,
      "tooltip": "The number of messages to send in one batch when using async mode",
      "qa": "fragment-details-kafka-batch-num-messages"
    },
    {
      "propertyId": "kafkaProperties",
      "propertyName": "_KAFKA_PROPERTIES_",
      "propertyType": "list",
      "default": "",
      "required": false,
      "tooltip": "",
      "qa": "fragment-details-kafka-properties",
      "fields": [
        {
          "propertyId": "kafkaPropertyKey",
          "propertyName": "_KAFKA_PROPERTY_KEY_",
          "propertyType": "text",
          "regexp": "",
          "default": "",
          "hidden": false,
          "required": false,
          "qa": "fragment-details-kafka-kafkaPropertyKey"
        },
        {
          "propertyId": "kafkaPropertyValue",
          "propertyName": "_KAFKA_PROPERTY_VALUE_",
          "propertyType": "text",
          "regexp": "",
          "default": "",
          "hidden": false,
          "required": false,
          "qa": "fragment-details-kafka-kafkaPropertyValue"
        }
      ]
    }
  ]
  },
  {
    "name": "Jdbc",
    "modelType": "Jdbc",
    "description": {
      "short": "With one jdbc connection is possible to write data into SQL Databases.",
      "long": "With one jdbc connection is possible to write data into SQL Databases. Using the generic implementation provided by Spark Datasources Api.",
      "learnMore": "http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources"
    },
    "properties": [
      {
        "propertyId": "url",
        "propertyName": "_JDBC_URL_",
        "propertyType": "text",
        "regexp": "",
        "default": "jdbc:postgresql:dbserver",
        "required": true,
        "tooltip": "Url to connect to one relational Database with jdbc",
        "qa": "fragment-details-jdbc-url"
      }
    ]
  },
  {
    "name": "Http",
    "modelType": "Http",
    "description": {
      "short": "Enables you to do POST requests to a certain URL",
      "long": "Enables you to do an HTTP POST request to a URL specifying if the data is to be passed through format with aparamter or attached in the request body.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-HTTPPost"
    },
    "properties": [
      {
        "propertyId": "url",
        "propertyName": "_URL_",
        "propertyType": "text",
        "regexp": "",
        "default": "",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-http-url"
      },
      {
        "propertyId": "connTimeout",
        "propertyName": "_CONN_TIMEOUT_",
        "propertyType": "text",
        "regexp": "",
        "default" : 1000,
        "required": false,
        "tooltip": "Value for the connection timeout. Default to 1000ms.",
        "qa": "fragment-details-http-output-conn-timeout"
      },
      {
        "propertyId": "readTimeout",
        "propertyName": "_READ_TIMEOUT_",
        "propertyType": "text",
        "regexp": "",
        "default" : 5000,
        "required": false,
        "tooltip": "Value for the read timeout. Default to 5000ms.",
        "qa": "fragment-details-http-output-read-timeout"
      },
      {
        "propertyId": "outputFormat",
        "propertyName": "_OUTPUT_FORMAT_",
        "propertyType": "select",
        "regexp": "json|row",
        "values": [
          {
            "label": "Json",
            "value": "json"
          },
          {
            "label": "Row",
            "value": "row"
          }
        ],
        "default": "json",
        "required": true,
        "tooltip": "Sets whether the output should be given as Json or as a row of String values",
        "qa": "fragment-details-http-output-format"
      },
      {
        "propertyId": "delimiter",
        "propertyName": "_DELIMITER_",
        "propertyType": "text",
        "tooltip": "Only eligible if Output Format is set to Row. Any character is accepted except: \\ \" #",
        "regexp": "[^\"#\\\\]",
        "maxLength": "1",
        "default": ",",
        "trim": false,
        "required": false,
        "qa": "fragment-details-http-delimiter"
      },
      {
        "propertyId": "postType",
        "propertyName": "_POST_TYPE_",
        "propertyType": "select",
        "regexp": "body|parameter",
        "values": [
          {
            "label": "Body",
            "value": "body"
          },
          {
            "label": "Parameter",
            "value": "parameter"
          }
        ],
        "default": "body",
        "required": true,
        "tooltip": "Sets whether the data should be send in the body or as a named parameter in a form",
        "qa": "fragment-details-http-post-type"
      },
      {
        "propertyId": "parameterName",
        "propertyName": "_PARAMETER_NAME_",
        "propertyType": "text",
        "regexp": "",
        "default" : "",
        "required": false,
        "tooltip": "Valid only if Post Type is set to parameter. Value entered will be used a key name.",
        "qa": "fragment-details-http-parameter-name"
      }
    ]
  },
  {
    "name": "FileSystem",
    "modelType": "FileSystem",
    "description": {
      "short": "Saves the processed data to file into any storage service",
      "long": "This output enables you to save the processed data into to a file into any storage service such as HDFS, Amazon S3, Azure or locally.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/SPARTA0x11/Outputs#Outputs-Filesystem"
    },
    "properties": [
      {
        "propertyId": "path",
        "propertyName": "_PATH_",
        "propertyType": "text",
        "regexp": "",
        "default": "",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-filesystem-path"
      },
      {
        "propertyId": "outputFormat",
        "propertyName": "_OUTPUT_FORMAT_",
        "propertyType": "select",
        "regexp": "json|row",
        "values": [
          {
            "label": "Json",
            "value": "json"
          },
          {
            "label": "Row",
            "value": "row"
          }
        ],
        "default": "json",
        "required": true,
        "tooltip": "Sets whether the output should be written as Json or plain text with just the row values.",
        "qa": "fragment-details-filesystem-format"
      },
      {
        "propertyId": "delimiter",
        "propertyName": "_DELIMITER_",
        "propertyType": "text",
        "tooltip": "Only eligible if output format is set to Row. Any character is accepted except: \\ \" #",
        "regexp": "[^\"#\\\\]",
        "maxLength": "1",
        "default": ",",
        "trim": false,
        "required": false,
        "qa": "fragment-details-filesystem-delimiter"
      },
      {
        "propertyId": "fileWithDate",
        "propertyName": "_FILE_WITH_DATE_",
        "propertyType": "select",
        "values": [
          {
            "label": "Yes",
            "value": "true"
          },
          {
            "label": "No",
            "value": "false"
          }
        ],
        "default": "true",
        "required": true,
        "tooltip": "Add the current date to the file name.",
        "qa": "fragment-details-filesystem-file-with-date"
      },
      {
        "propertyId": "dateFormat",
        "propertyName": "_DATE_FORMAT_",
        "propertyType": "select",
        "regexp": "",
        "values": [
          {
            "label": "yyyy-mm-dd'-'HH.mm",
            "value": "YYYY-MM-DD'-'HH.mm"
          },
          {
            "label": "yyyy-mm-dd'-'HH.mm.ss",
            "value": "YYYY-MM-DD'-'HH.mm.ss"
          },
          {
            "label": "yyyy-mm-dd'-'HH.mmZ",
            "value": "YYYY-MM-DD'-'HH.mmZ"
          },
          {
            "label": "yyyy-mm-dd'-'HH.mm.ssZ",
            "value": "YYYY-MM-DD'-'HH.mm.ssZ"
          },
          {
            "label": "yyyy-mm-dd'T'HH.mm",
            "value": "YYYY-MM-DD'T'HH.mm"
          },
          {
            "label": "yyyy-mm-dd'T'HH.mmZ",
            "value": "YYYY-MM-DD'T'HH.mmZ"
          },
          {
            "label": "yyyy-mm-dd'T'HH.mm.ss",
            "value": "YYYY-MM-DD'T'HH.mm.ss"
          },
          {
            "label": "yyyy-mm-dd'T'HH.mm.ssZ",
            "value": "YYYY-MM-DD'T'HH.mm.ssZ"
          },
          {
            "label": "yyyy-mm-dd",
            "value": "YYYY-MM-DD"
          },
          {
            "label": "yyyy-mm-ddZ",
            "value": "YYYY-MM-DDZ"
          }
        ],
        "default": "YYYY-MM-DD'-'HH.mm.ss",
        "required": false,
        "tooltip": "The date format to be appended to the file name. Only used if Append date is set to yes.",
        "qa": "fragment-details-filesystem-date-format"
      },
      {
        "propertyId": "partitionUntil",
        "propertyName": "_PARTITION_UNTIL_",
        "propertyType": "select",
        "regexp": "",
        "values": [
          {
            "label": "Year",
            "value": "YYYY"
          },
          {
            "label": "Month",
            "value": "MM"
          },
          {
            "label": "Day",
            "value": "DD"
          },
          {
            "label": "Hour",
            "value": "HH"
          },
          {
            "label": "Minutes",
            "value": "mm"
          },
          {
            "label": "Seconds",
            "value": "ss"
          },
          {
            "label": "No partition",
            "value": "NONE"
          }
        ],
        "default": "NONE",
        "required": false,
        "tooltip": "Sets the limit the date will be partitioned, creating sub-folders inside the specified the given path",
        "qa": "fragment-details-filesystem--partition-until"
      }
    ]
  },
  {
    "name": "Avro",
    "modelType": "Avro",
    "description": {
      "short": "Avro output uses the databricks implementation with DataFrames.",
      "long": "Avro is a remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. Its primary use is in Apache Hadoop, where it can provide both a serialization format for persistent data, and a wire format for communication between Hadoop nodes, and from client programs to the Hadoop services.",
      "learnMore": "https://stratio.atlassian.net/wiki/display/latSPARTA0x11est/Outputs#Outputs-Avro"
    },
    "properties": [
      {
        "propertyId": "path",
        "propertyName": "_PATH_",
        "propertyType": "text",
        "regexp": "",
        "default": "",
        "required": true,
        "tooltip": "",
        "qa": "fragment-details-avro-path"
      },
      {
        "propertyId": "partitionBy",
        "propertyName": "_PARTITION_BY_",
        "propertyType": "text",
        "regexp": "",
        "required": false,
        "tooltip": "",
        "qa": "fragment-details-avro-partitionBy"
      }
    ]
  }
]
